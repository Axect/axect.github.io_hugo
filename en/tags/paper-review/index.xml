<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>paper-review on Axect&#39;s Blog</title>
    <link>https://axect.github.io/en/tags/paper-review/</link>
    <description>Recent content in paper-review on Axect&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 29 Oct 2022 17:39:54 +0900</lastBuildDate>
    <atom:link href="https://axect.github.io/en/tags/paper-review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ðŸ’” Decorrelation &#43; Deep learning = Generalization</title>
      <link>https://axect.github.io/en/posts/005_decov/</link>
      <pubDate>Sat, 29 Oct 2022 17:39:54 +0900</pubDate>
      <guid>https://axect.github.io/en/posts/005_decov/</guid>
      <description>arXiv: 1511.06068
The most pervasive challenge in deep learning is Overfitting . This occurs when the dataset is small, and extensive training leads to a model that excels on training datasets but fails to generalize to validation datasets or real-world scenarios. To address this issue, various strategies have been developed. Historically, in statistics, regularization methods like Ridge and LASSO were employed, while deep learning has adopted approaches such as regularizing weights or applying different techniques to neural networks.</description>
    </item>
  </channel>
</rss>
