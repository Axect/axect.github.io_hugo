<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch on Axect&#39;s Blog</title>
    <link>https://axect.github.io/tags/pytorch/</link>
    <description>Recent content in pytorch on Axect&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 29 Oct 2022 17:39:54 +0900</lastBuildDate><atom:link href="https://axect.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>💔 Decorrelation &#43; Deep learning = Generalization</title>
      <link>https://axect.github.io/posts/005_decov/</link>
      <pubDate>Sat, 29 Oct 2022 17:39:54 +0900</pubDate>
      
      <guid>https://axect.github.io/posts/005_decov/</guid>
      <description>arXiv: 1511.06068
딥러닝에서 가장 빈번하게 일어나는 문제로 Overfitting (과적합) 이 있습니다. 이는 데이터가 많지 않을 때, 학습을 많이 할 수록 잘 발생하는 문제이며 이로 인하여 훈련 데이터셋에 대해서는 성능이 좋더라도 검증 데이터셋이나 실제 데이터셋에 대해서는 성능이 안 나오는 문제가 발생합니다. 이를 해결하기 위하여 사람들은 여러 방법을 고안했는데, 통계학에서는 일찌감치 Ridge나 LASSO와 같은 regularization 방법을 사용하였으며 딥러닝에서도 마찬가지로 weight을 regularize하거나 신경망에 여러 기술을 적용하는 것들을 도입하였습니다. 이러한 기술로는 다음과 같은 방법들이 있습니다.</description>
    </item>
    
  </channel>
</rss>
